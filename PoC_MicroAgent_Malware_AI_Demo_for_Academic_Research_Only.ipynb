{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhYLwAyDg3V/Z8bIirflTG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elangbijak4/Offensive-Defensive-AI-Framework-for-CyberSecurity/blob/main/PoC_MicroAgent_Malware_AI_Demo_for_Academic_Research_Only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35gwy5lLNjVL",
        "outputId": "1e6567c5-360b-4e4f-e889-cb41aa72f3c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from generated code\n",
            "Python version: 3.12.12\n"
          ]
        }
      ],
      "source": [
        "# Benign PoC: Local Micro-Agent (Colab safe demo)\n",
        "# - No filesystem writes\n",
        "# - No subprocess/os.system\n",
        "# - No network calls (fake_llm_server is local)\n",
        "# - Simple policy check before exec\n",
        "\n",
        "import base64\n",
        "import json\n",
        "import platform\n",
        "from textwrap import dedent\n",
        "\n",
        "###########################\n",
        "# 1) Encrypted Prompt (Base64 - simulation of obfuscation)\n",
        "###########################\n",
        "plain_prompt_template = dedent(\"\"\"\n",
        "You are a safe Python assistant.\n",
        "\n",
        "You receive this JSON describing the environment:\n",
        "{env_json}\n",
        "\n",
        "Task:\n",
        "Write VALID Python code that defines a function called `generated_main()`.\n",
        "When called, this function must:\n",
        " - Print the string \"Hello from generated code\".\n",
        " - Print the Python version using the platform module.\n",
        "\n",
        "Important:\n",
        " - Only output pure Python code.\n",
        " - Do NOT include imports other than `platform`.\n",
        " - Do NOT use os, sys, subprocess, socket, open, eval, exec, importlib, ctypes.\n",
        "\"\"\").strip()\n",
        "\n",
        "# simulate obfuscation/encryption\n",
        "encrypted_prompt = base64.b64encode(plain_prompt_template.encode('utf-8')).decode('utf-8')\n",
        "\n",
        "###########################\n",
        "# 2) Local Micro-Agent primitives\n",
        "###########################\n",
        "def decrypt_prompt(enc_str: str) -> str:\n",
        "    \"\"\"Decode Base64 'encrypted' prompt back to plain text.\"\"\"\n",
        "    return base64.b64decode(enc_str.encode('utf-8')).decode('utf-8')\n",
        "\n",
        "def collect_environment_info() -> dict:\n",
        "    \"\"\"Collect only non-sensitive environment info.\"\"\"\n",
        "    return {\n",
        "        \"python_version\": platform.python_version(),\n",
        "        \"system\": platform.system(),\n",
        "        \"release\": platform.release(),\n",
        "        \"machine\": platform.machine()\n",
        "    }\n",
        "\n",
        "def fill_prompt_template(template: str, env: dict) -> str:\n",
        "    env_json = json.dumps(env, indent=2)\n",
        "    return template.format(env_json=env_json)\n",
        "\n",
        "###########################\n",
        "# 3) Fake LLM Server (deterministic, safe)\n",
        "###########################\n",
        "def fake_llm_server(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Simulated LLM: returns a harmless Python code string.\n",
        "    In real experiments, researchers may replace this with an API call\n",
        "    to a controlled LLM endpoint that enforces safety policies.\n",
        "    \"\"\"\n",
        "    # A deliberately simple and explicit safe code block\n",
        "    code = dedent(\"\"\"\n",
        "    import platform\n",
        "\n",
        "    def generated_main():\n",
        "        print(\"Hello from generated code\")\n",
        "        print(\"Python version:\", platform.python_version())\n",
        "    \"\"\").strip()\n",
        "    return code\n",
        "\n",
        "###########################\n",
        "# 4) Safety Policy Check before execution\n",
        "###########################\n",
        "FORBIDDEN_TOKENS = [\n",
        "    \"os.system\", \"subprocess\", \"socket\", \"open(\", \"eval(\", \"exec(\",\n",
        "    \"import os\", \"import sys\", \"ctypes\", \"importlib\", \"from os\", \"from sys\",\n",
        "    \"__import__\", \"shutil\", \"fork\", \"popen\"\n",
        "]\n",
        "\n",
        "def safe_policy_check(code_str: str) -> None:\n",
        "    \"\"\"Raise RuntimeError if code contains forbidden tokens.\"\"\"\n",
        "    lower = code_str.lower()\n",
        "    for token in FORBIDDEN_TOKENS:\n",
        "        if token in lower:\n",
        "            raise RuntimeError(f\"Unsafe token detected in generated code: {token}\")\n",
        "\n",
        "###########################\n",
        "# 5) Execute in restricted namespace\n",
        "###########################\n",
        "def execute_generated_code(code_str):\n",
        "    \"\"\"\n",
        "    Execute code in a restricted namespace.\n",
        "    We pass an empty globals dict to reduce possible access to outer scope.\n",
        "    The local namespace receives only what exec defines.\n",
        "    \"\"\"\n",
        "    # Policy check\n",
        "    safe_policy_check(code_str)\n",
        "\n",
        "    # Prepare very restricted globals (only builtins allowed)\n",
        "    restricted_globals = {\n",
        "        \"__builtins__\": {\n",
        "            # allow only a minimal safe subset of builtins\n",
        "            \"True\": True, \"False\": False, \"None\": None, \"print\": print,\n",
        "            \"len\": len, \"range\": range, \"__import__\": __import__\n",
        "        },\n",
        "        \"platform\": platform # Explicitly add the platform module to the restricted globals\n",
        "    }\n",
        "\n",
        "    local_ns = {}\n",
        "    exec(code_str, restricted_globals, local_ns)\n",
        "    if \"generated_main\" not in local_ns:\n",
        "        raise RuntimeError(\"generated_main not found in generated code\")\n",
        "    # Call the generated function\n",
        "    local_ns[\"generated_main\"]()\n",
        "\n",
        "###########################\n",
        "# 6) Full agent run (simulate)\n",
        "###########################\n",
        "def run_local_agent_demo(enc_prompt: str):\n",
        "    # 1. decrypt\n",
        "    template = decrypt_prompt(enc_prompt)\n",
        "    # 2. collect environment\n",
        "    env = collect_environment_info()\n",
        "    # 3. fill template\n",
        "    filled_prompt = fill_prompt_template(template, env)\n",
        "    # 4. send to fake LLM server\n",
        "    generated_code = fake_llm_server(filled_prompt)\n",
        "    # 5. execute (with safe policies)\n",
        "    execute_generated_code(generated_code)\n",
        "\n",
        "# Run demo\n",
        "if __name__ == \"__main__\":\n",
        "    run_local_agent_demo(encrypted_prompt)"
      ]
    }
  ]
}